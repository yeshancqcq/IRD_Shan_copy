knitr::opts_chunk$set(echo = TRUE, message=FALSE, warning = FALSE)
install.packages("knitr")
knitr::opts_chunk$set(echo = TRUE, message=FALSE, warning = FALSE)
sentence <- "this,is,a,IRD,and,CNRS,sentence,you,didny,want,."
stringr::str_detect(sentence, "IRD") & !stringr::str_detect(sentence, "CNRS")
howmany_dates <- all_sentences %>%
mutate(hasAge = stringr::str_detect(words, "regular expression for dates")) %>%
group_by(gddid) %>%
summarise(age_sentences = any(hasAge),
n = n())
#devtools::install_github('EarthCubeGeochron/geodiveR')
library(geodiveR)
install.packages("geodiveR")
devtools::install_github('EarthCubeGeochron/geodiveR')
#devtools::install_github('EarthCubeGeochron/geodiveR')
library(geodiveR)
install.packages("rlang")
knitr::opts_chunk$set(echo = TRUE, message=FALSE, warning = FALSE)
#devtools::install_github('EarthCubeGeochron/geodiveR')
library(geodiveR)
library(jsonlite)
library(readr)
library(dplyr)
library(stringr)
library(leaflet)
install.packages("leaflet")
#devtools::install_github('EarthCubeGeochron/geodiveR')
library(geodiveR)
library(jsonlite)
library(readr)
library(dplyr)
library(stringr)
library(leaflet)
library(purrr)
library(DT)
library(assertthat)
sourcing <- list.files('R', full.names = TRUE) %>%
map(source, echo = FALSE, print = FALSE, verbose = FALSE)
publications <- fromJSON(txt = 'input/bibjson', flatten = TRUE)
full_nlp <- readr::read_tsv('input/sentences_nlp352',
trim_ws = TRUE,
col_names = c('_gddid', 'sentence', 'wordIndex',
'word', 'partofspeech', 'specialclass',
'wordsAgain', 'wordtype', 'wordmodified'))
nlp_clean <- clean_corpus(x = full_nlp, pubs = publications) #uses the clean_corpus.R function
nlp<-nlp_clean$nlp
short_table <- nlp  %>%
filter(1:nrow(nlp) %in% 1) %>%
str_replace('-LRB-', '(') %>%
str_replace('-RRB-', ')') %>%
as.data.frame(stringsAsFactors = FALSE)
rownames(short_table) <- colnames(nlp_clean)
colnames(short_table) <- 'value'
short_table[nchar(short_table[,1])>40,1] <-
paste0('<code>', substr(short_table[nchar(short_table[,1])>40, 1], 1, 30), ' ... }</code>')
rownames(short_table) <- colnames(nlp)
short_table$description <- c("Unique article identifier",
"Unique sentence identifier within article",
"Index of words within sentence",
"Verbatim OCR word",
"Parts of speech, based on <a href='https://www.ling.upenn.edu/courses/Fall_2003/ling001/penn_treebank_pos.html'>Penn State TreeView</a>",
"Special classes (numbers, dates, &cetera)",
"Words again",
"Word types, based on <a href='http://universaldependencies.org/introduction.html'>universal dependencies</a>.",
"The word that the <code>wordtype</code> is modifying.")
short_table %>%
datatable(escape = FALSE, rownames = TRUE, options = list(dom='t'))
dms_regex <- "[\\{,]([-]?[1]?[0-9]{1,2}?)(?:(?:,[°◦o],)|(?:[O])|(?:,`{2},))([1]?[0-9]{1,2}(?:.[0-9]*)),[′'`]?[,]?([[0-9]{0,2}]?)[\"]?[,]?([NESWnesw]?),"
dd_regex <- "[\\{,][-]?[1]?[0-9]{1,2}\\.[0-9]{1,}[,]?[NESWnesw],"
#dd_regex <- "[\\{,][-]?[1]?[0-9]{1,2}\\.[0-9]{1,}[,]?[NESWnesw],"
#dms_regex <- "[\\{,]([-]?[1]?[0-9]{1,2}?)(?:(?:,[°◦oºø],)|(?:[O])|(?:,`{2},))([1]?[0-9]{1,3}(?:.[0-9]*)),[´′'`]?[,]?([[0-9]{0,2}]?)[\"]?[,]?([NESWnesw]?),"
degmin <- str_match_all(nlp$word, dms_regex)
decdeg <- str_match_all(nlp$word, dd_regex)
coord_pairs <- sapply(degmin, function(x)length(x) %% 2 == 0 & length(x) > 0) |
sapply(decdeg, function(x)length(x) %% 2 == 0 & length(x) > 0)
things <- nlp %>%
filter(coord_pairs) %>%
inner_join(publications, by = "_gddid") %>%
select(`_gddid`, word, year, title) %>%
mutate(word = gsub(',', ' ', word)) %>%
mutate(word = str_replace_all(word, '-LRB-', '(')) %>%
mutate(word = str_replace_all(word, '-RRB-', ')')) %>%
mutate(word = str_replace_all(word, '" "', ','))
things %>% select(-`_gddid`) %>% datatable
convert_dec <- function(x, i) {
drop_comma <- gsub(',', '', x) %>%
substr(., c(1,1), nchar(.) - 1) %>%
as.numeric %>%
unlist
domain <- (str_detect(x, 'N') * 1 +
str_detect(x, 'E') * 1 +
str_detect(x, 'W') * -1 +
str_detect(x, 'S') * -1) *
drop_comma
publ <- match(nlp$`_gddid`[i], publications$`_gddid`)
point_pairs <- data.frame(sentence = nlp$word[i],
lat = domain[str_detect(x, 'N') | str_detect(x, 'S')],
lng = domain[str_detect(x, 'E') | str_detect(x, 'W')],
publications[publ,],
stringsAsFactors = FALSE)
return(point_pairs)
}
convert_dm <- function(x, i) {
# We use the `i` index so that we can keep the coordinate outputs from the
#  regex in a smaller list.
dms <- data.frame(deg = as.numeric(x[,2]),
min = as.numeric(x[,3]) / 60,
sec = as.numeric(x[,4]) / 60 ^ 2,
stringsAsFactors = FALSE)
dms <- rowSums(dms, na.rm = TRUE)
domain <- (str_detect(x[,5], 'N') * 1 +
str_detect(x[,5], 'E') * 1 +
str_detect(x[,5], 'W') * -1 +
str_detect(x[,5], 'S') * -1) *
dms
publ <- match(nlp$`_gddid`[i], publications$`_gddid`)
point_pairs <- data.frame(sentence = nlp$word[i],
lat = domain[x[,5] %in% c('N', 'S')],
lng = domain[x[,5] %in% c('E', 'W')],
publications[publ,],
stringsAsFactors = FALSE)
return(point_pairs)
}
coordinates <- list()
coord_idx <- 1
for(i in 1:length(decdeg)) {
if((length(decdeg[[i]]) %% 2 == 0 |
length(degmin[[i]]) %% 2 == 0) & length(degmin[[i]]) > 0) {
if(any(str_detect(decdeg[[i]], '[NS]')) &
sum(str_detect(decdeg[[i]], '[EW]')) == sum(str_detect(decdeg[[i]], '[NS]'))) {
coordinates[[coord_idx]] <- convert_dec(decdeg[[i]], i)
coord_idx <- coord_idx + 1
}
if(any(str_detect(degmin[[i]], '[NS]')) &
sum(str_detect(degmin[[i]], '[EW]')) == sum(str_detect(degmin[[i]], '[NS]'))) {
coordinates[[coord_idx]] <- convert_dm(degmin[[i]], i)
coord_idx <- coord_idx + 1
}
}
}
coordinates_df <- coordinates %>% bind_rows %>%
mutate(sentence = gsub(',', ' ', sentence)) %>%
mutate(sentence = str_replace_all(sentence, '-LRB-', '(')) %>%
mutate(sentence = str_replace_all(sentence, '-RRB-', ')')) %>%
mutate(sentence = str_replace_all(sentence, '" "', ','))
coordinates_df$doi <- coordinates_df$identifier %>% map(function(x) x$id) %>% unlist
leaflet(coordinates_df) %>%
addProviderTiles(providers$CartoDB.Positron) %>%
addCircleMarkers(popup = paste0('<b>', coordinates_df$title, '</b><br>',
'<a href=https://doi.org/',
coordinates_df$doi,'>Publication Link</a><br>',
'<b>Sentence:</b><br>',
'<small>',gsub(',', ' ', coordinates_df$sentence),
'</small>'))
View(coordinates_df)
View(things)
View(publications)
View(full_nlp)
View(nlp_clean)
View(nlp)
coord_pairs <- sapply(degmin, function(x)length(x) %% 2 == 0 & length(x) > 0) |
sapply(decdeg, function(x)length(x) %% 2 == 0 & length(x) > 0)
things <- nlp %>%
filter(coord_pairs) %>%
inner_join(publications, by = "_gddid") %>%
select(`_gddid`, word, year, title, sentence) %>%
mutate(word = gsub(',', ' ', word)) %>%
mutate(word = str_replace_all(word, '-LRB-', '(')) %>%
mutate(word = str_replace_all(word, '-RRB-', ')')) %>%
mutate(word = str_replace_all(word, '" "', ','))
things %>% select(-`_gddid`) %>% datatable
convert_dec <- function(x, i) {
drop_comma <- gsub(',', '', x) %>%
substr(., c(1,1), nchar(.) - 1) %>%
as.numeric %>%
unlist
domain <- (str_detect(x, 'N') * 1 +
str_detect(x, 'E') * 1 +
str_detect(x, 'W') * -1 +
str_detect(x, 'S') * -1) *
drop_comma
publ <- match(nlp$`_gddid`[i], publications$`_gddid`)
point_pairs <- data.frame(sentence = nlp$word[i],
lat = domain[str_detect(x, 'N') | str_detect(x, 'S')],
lng = domain[str_detect(x, 'E') | str_detect(x, 'W')],
publications[publ,],
stringsAsFactors = FALSE)
return(point_pairs)
}
convert_dm <- function(x, i) {
# We use the `i` index so that we can keep the coordinate outputs from the
#  regex in a smaller list.
dms <- data.frame(deg = as.numeric(x[,2]),
min = as.numeric(x[,3]) / 60,
sec = as.numeric(x[,4]) / 60 ^ 2,
stringsAsFactors = FALSE)
dms <- rowSums(dms, na.rm = TRUE)
domain <- (str_detect(x[,5], 'N') * 1 +
str_detect(x[,5], 'E') * 1 +
str_detect(x[,5], 'W') * -1 +
str_detect(x[,5], 'S') * -1) *
dms
publ <- match(nlp$`_gddid`[i], publications$`_gddid`)
point_pairs <- data.frame(sentence = nlp$word[i],
lat = domain[x[,5] %in% c('N', 'S')],
lng = domain[x[,5] %in% c('E', 'W')],
publications[publ,],
stringsAsFactors = FALSE)
return(point_pairs)
}
coordinates <- list()
coord_idx <- 1
for(i in 1:length(decdeg)) {
if((length(decdeg[[i]]) %% 2 == 0 |
length(degmin[[i]]) %% 2 == 0) & length(degmin[[i]]) > 0) {
if(any(str_detect(decdeg[[i]], '[NS]')) &
sum(str_detect(decdeg[[i]], '[EW]')) == sum(str_detect(decdeg[[i]], '[NS]'))) {
coordinates[[coord_idx]] <- convert_dec(decdeg[[i]], i)
coord_idx <- coord_idx + 1
}
if(any(str_detect(degmin[[i]], '[NS]')) &
sum(str_detect(degmin[[i]], '[EW]')) == sum(str_detect(degmin[[i]], '[NS]'))) {
coordinates[[coord_idx]] <- convert_dm(degmin[[i]], i)
coord_idx <- coord_idx + 1
}
}
}
coordinates_df <- coordinates %>% bind_rows %>%
mutate(sentence = gsub(',', ' ', sentence)) %>%
mutate(sentence = str_replace_all(sentence, '-LRB-', '(')) %>%
mutate(sentence = str_replace_all(sentence, '-RRB-', ')')) %>%
mutate(sentence = str_replace_all(sentence, '" "', ','))
coordinates_df$doi <- coordinates_df$identifier %>% map(function(x) x$id) %>% unlist
leaflet(coordinates_df) %>%
addProviderTiles(providers$CartoDB.Positron) %>%
addCircleMarkers(popup = paste0('<b>', coordinates_df$title, '</b><br>',
'<a href=https://doi.org/',
coordinates_df$doi,'>Publication Link</a><br>',
'<b>Sentence:</b><br>',
'<small>',gsub(',', ' ', coordinates_df$sentence),
'</small>'))
View(coordinates_df)
View(things)
coord_pairs <- sapply(degmin, function(x)length(x) %% 2 == 0 & length(x) > 0) |
sapply(decdeg, function(x)length(x) %% 2 == 0 & length(x) > 0)
things <- nlp %>%
filter(coord_pairs) %>%
inner_join(publications, by = "_gddid") %>%
select(`_gddid`, word, year, title, sentence) %>%
mutate(word = gsub(',', ' ', word)) %>%
mutate(word = str_replace_all(word, '-LRB-', '(')) %>%
mutate(word = str_replace_all(word, '-RRB-', ')')) %>%
mutate(word = str_replace_all(word, '" "', ','))
things %>% select(-`_gddid`) %>% datatable
convert_dec <- function(x, i) {
drop_comma <- gsub(',', '', x) %>%
substr(., c(1,1), nchar(.) - 1) %>%
as.numeric %>%
unlist
domain <- (str_detect(x, 'N') * 1 +
str_detect(x, 'E') * 1 +
str_detect(x, 'W') * -1 +
str_detect(x, 'S') * -1) *
drop_comma
publ <- match(nlp$`_gddid`[i], publications$`_gddid`)
point_pairs <- data.frame(sentence = nlp$word[i],
sentence_num = nlp$sentence[i],
lat = domain[str_detect(x, 'N') | str_detect(x, 'S')],
lng = domain[str_detect(x, 'E') | str_detect(x, 'W')],
publications[publ,],
stringsAsFactors = FALSE)
return(point_pairs)
}
convert_dm <- function(x, i) {
# We use the `i` index so that we can keep the coordinate outputs from the
#  regex in a smaller list.
dms <- data.frame(deg = as.numeric(x[,2]),
min = as.numeric(x[,3]) / 60,
sec = as.numeric(x[,4]) / 60 ^ 2,
stringsAsFactors = FALSE)
dms <- rowSums(dms, na.rm = TRUE)
domain <- (str_detect(x[,5], 'N') * 1 +
str_detect(x[,5], 'E') * 1 +
str_detect(x[,5], 'W') * -1 +
str_detect(x[,5], 'S') * -1) *
dms
publ <- match(nlp$`_gddid`[i], publications$`_gddid`)
point_pairs <- data.frame(sentence = nlp$word[i],
sentence_num = nlp$sentence[i],
lat = domain[x[,5] %in% c('N', 'S')],
lng = domain[x[,5] %in% c('E', 'W')],
publications[publ,],
stringsAsFactors = FALSE)
return(point_pairs)
}
coordinates <- list()
coord_idx <- 1
for(i in 1:length(decdeg)) {
if((length(decdeg[[i]]) %% 2 == 0 |
length(degmin[[i]]) %% 2 == 0) & length(degmin[[i]]) > 0) {
if(any(str_detect(decdeg[[i]], '[NS]')) &
sum(str_detect(decdeg[[i]], '[EW]')) == sum(str_detect(decdeg[[i]], '[NS]'))) {
coordinates[[coord_idx]] <- convert_dec(decdeg[[i]], i)
coord_idx <- coord_idx + 1
}
if(any(str_detect(degmin[[i]], '[NS]')) &
sum(str_detect(degmin[[i]], '[EW]')) == sum(str_detect(degmin[[i]], '[NS]'))) {
coordinates[[coord_idx]] <- convert_dm(degmin[[i]], i)
coord_idx <- coord_idx + 1
}
}
}
coordinates_df <- coordinates %>% bind_rows %>%
mutate(sentence = gsub(',', ' ', sentence)) %>%
mutate(sentence = str_replace_all(sentence, '-LRB-', '(')) %>%
mutate(sentence = str_replace_all(sentence, '-RRB-', ')')) %>%
mutate(sentence = str_replace_all(sentence, '" "', ','))
coordinates_df$doi <- coordinates_df$identifier %>% map(function(x) x$id) %>% unlist
leaflet(coordinates_df) %>%
addProviderTiles(providers$CartoDB.Positron) %>%
addCircleMarkers(popup = paste0('<b>', coordinates_df$title, '</b><br>',
'<a href=https://doi.org/',
coordinates_df$doi,'>Publication Link</a><br>',
'<b>Sentence:</b><br>',
'<small>',gsub(',', ' ', coordinates_df$sentence),
'</small>'))
View(coordinates_df)
output_df <- data.frame()
output_df <- data.frame(
sentence_num = coordinates_df$sentence_num,
sentence = coordinates_df$sentence,
lat = coordinates_df$lat,
lon = coordinates_df$lng
)
write.csv(output_df, file = "output_sentence_number.csv")
knitr::opts_chunk$set(echo = FALSE, warning=FALSE, message=FALSE)
