x <- 5
file <- nlp_test
# Copy the input NLP data.frame to a temporary data.frame
group_nlp = NULL
group_nlp <- file
# Build an empty data frame with 2 columns: _gddid and group_id
df = NULL
df <- data.frame(matrix(vector(), nrow=nrow(group_nlp) ,ncol=3))
colnames(df) <-c("_gddid","sentence","group_id")
# Copy the _gddid from file to df
df$`_gddid` <- group_nlp$`_gddid`
df$sentence <- group_nlp$sentence
# add values to the counter column and they will be keys for aggregation later
counter <- 2
placement <- 1
group_id <- 1
df$group_id[1] <- group_id
placement <- placement + 1
while (counter < nrow(group_nlp) +1){
if (isTRUE(df$`_gddid`[counter] == df$`_gddid`[counter - 1]) & placement <= number_of_sentence) {
df$group_id[counter] <- group_id
counter <- counter + 1
placement <- placement + 1
}
else if (isTRUE(df$`_gddid`[counter] == df$`_gddid`[counter - 1]) & placement > number_of_sentence) {
placement <-1
group_id <- group_id + 1
df$group_id[counter] <- group_id
counter <- counter + 1
placement <- placement + 1
}
else {
group_id <- group_id + 1
df$group_id[counter] <- group_id
placement <- 1
counter <- counter + 1
}
dt_new <- merge(df,group_nlp,by=c("_gddid","sentence"))
View(df)
View(df)
View(df)
View(df)
knitr::opts_chunk$set(echo = TRUE, message=FALSE, warning = FALSE)
devtools::install_github('EarthCubeGeochron/geodiveR')
library(geodiveR)
library(jsonlite)
library(readr)
library(dplyr)
library(stringr)
library(leaflet)
library(purrr)
library(DT)
library(assertthat)
library(data.table)
sourcing <- list.files('R', full.names = TRUE) %>%
map(source, echo = FALSE, print = FALSE, verbose = FALSE)
publications_test <- fromJSON(txt = 'input/bibjson', flatten = TRUE)
full_nlp_test <- readr::read_tsv('input/sentences_nlp352',
trim_ws = TRUE,
col_names = c('_gddid', 'sentence', 'wordIndex',
'word', 'partofspeech', 'specialclass',
'wordsAgain', 'wordtype', 'wordmodified'))
nlp_clean_test <- clean_corpus(x = full_nlp_test, pubs = publications_test) #uses the clean_corpus.R function
nlp_test<-nlp_clean_test$nlp
x <- 5
file <- nlp_test
# Copy the input NLP data.frame to a temporary data.frame
group_nlp = NULL
group_nlp <- file
df = NULL
df <- data.frame(matrix(vector(), nrow=nrow(group_nlp) ,ncol=3))
colnames(df) <-c("_gddid","sentence","group_id")
# Copy the _gddid from file to df
df$`_gddid` <- group_nlp$`_gddid`
df$sentence <- group_nlp$sentence
# add values to the counter column and they will be keys for aggregation later
counter <- 2
placement <- 1
group_id <- 1
df$group_id[1] <- group_id
placement <- placement + 1
while (counter < nrow(group_nlp) +1){
if (isTRUE(df$`_gddid`[counter] == df$`_gddid`[counter - 1]) & placement <= number_of_sentence) {
df$group_id[counter] <- group_id
counter <- counter + 1
placement <- placement + 1
}
else if (isTRUE(df$`_gddid`[counter] == df$`_gddid`[counter - 1]) & placement > number_of_sentence) {
placement <-1
group_id <- group_id + 1
df$group_id[counter] <- group_id
counter <- counter + 1
placement <- placement + 1
}
else {
group_id <- group_id + 1
df$group_id[counter] <- group_id
placement <- 1
counter <- counter + 1
}
}
number_of_sentence <- 5
file <- nlp_test
# Copy the input NLP data.frame to a temporary data.frame
group_nlp = NULL
group_nlp <- file
# Build an empty data frame with 2 columns: _gddid and group_id
df = NULL
df <- data.frame(matrix(vector(), nrow=nrow(group_nlp) ,ncol=3))
colnames(df) <-c("_gddid","sentence","group_id")
# Copy the _gddid from file to df
df$`_gddid` <- group_nlp$`_gddid`
df$sentence <- group_nlp$sentence
# add values to the counter column and they will be keys for aggregation later
counter <- 2
placement <- 1
group_id <- 1
df$group_id[1] <- group_id
placement <- placement + 1
while (counter < nrow(group_nlp) +1){
if (isTRUE(df$`_gddid`[counter] == df$`_gddid`[counter - 1]) & placement <= number_of_sentence) {
df$group_id[counter] <- group_id
counter <- counter + 1
placement <- placement + 1
}
else if (isTRUE(df$`_gddid`[counter] == df$`_gddid`[counter - 1]) & placement > number_of_sentence) {
placement <-1
group_id <- group_id + 1
df$group_id[counter] <- group_id
counter <- counter + 1
placement <- placement + 1
}
else {
group_id <- group_id + 1
df$group_id[counter] <- group_id
placement <- 1
counter <- counter + 1
}
}
View(df)
dt_new <- merge(df,group_nlp,by=c("_gddid","sentence"))
View(dt_new)
View(dt_new)
output_df <-  aggregate(.~group_id, dt_new, paste, collapse = ",")
View(output_df)
output_df <- aggregate(id ~ group_id, data = dt_new, c)
output_df <- aggregate(.~ group_id, data = dt_new, c)
View(output_df)
output_df <- aggregate(.~ group_id, data = dt_new, paste)
View(output_df)
output_df <-  aggregate(.~group_id, dt_new, paste, collapse = ",")
View(output_df)
for (i in 1:nrow(output_df)){
output_df$`_gddid`[i] <- gsub("(.*),.*", "\\1", output_df$`_gddid`[i])
}
View(output_df)
output_df <-  aggregate(.~group_id, dt_new, paste, collapse = ",")
View(output_df)
for (i in 1:nrow(output_df)){
output_df$`_gddid`[i] <- gsub(",.*", "", output_df$`_gddid`[i])
}
View(output_df)
devtools::install_github('EarthCubeGeochron/geodiveR')
library(geodiveR)
library(jsonlite)
library(readr)
library(dplyr)
library(stringr)
library(leaflet)
library(purrr)
library(DT)
library(assertthat)
library(data.table)
sourcing <- list.files('R', full.names = TRUE) %>%
map(source, echo = FALSE, print = FALSE, verbose = FALSE)
publications_test <- fromJSON(txt = 'input/bibjson', flatten = TRUE)
full_nlp_test <- readr::read_tsv('input/sentences_nlp352',
trim_ws = TRUE,
col_names = c('_gddid', 'sentence', 'wordIndex',
'word', 'partofspeech', 'specialclass',
'wordsAgain', 'wordtype', 'wordmodified'))
nlp_clean_test <- clean_corpus(x = full_nlp_test, pubs = publications_test) #uses the clean_corpus.R function
nlp_test<-nlp_clean_test$nlp
test_group <- NULL
test_group <- nlp_test
#Aggregate every 5 sentences
test_group_5s = NULL
test_group_5s <- groupNLP(test_group,5)
# Need to fix the data structure of groupNLP loop, below is only temporary
# test_group_5s <- output_df
nlp_group_test <- test_group_5s
View(test_group_5s)
short_table_test <- nlp_group_test  %>%
filter(1:9226 %in% 1) %>%
str_replace('-LRB-', '(') %>%
str_replace('-RRB-', ')') %>%
as.data.frame(stringsAsFactors = FALSE)
rownames(short_table_test) <- colnames(nlp_clean_test)
colnames(short_table_test) <- 'value'
short_table_test[nchar(short_table_test[,1])>40,1] <-
paste0(substr(short_table_test[nchar(short_table_test[,1])>40, 1], 1, 30), ' . . .')
short_table_test %>% datatable()
dms_regex_test <- "[\\{,]([-]?[1]?[0-9]{1,2}?)(?:(?:,[°◦o],)|(?:[O])|(?:,`{2},))([1]?[0-9]{1,2}(?:.[0-9]*)),[′'`]?[,]?([[0-9]{0,2}]?)[\"]?[,]?([NESWnesw]?),"
#dms_regex <- "[\\{,]([-]?[1]?[0-9]{1,2}?)(?:(?:,[°◦oºø],)|(?:[O])|(?:,`{2},))([1]?[0-9]{1,3}(?:.[0-9]*)),[´′'`]?[,]?([[0-9]{0,2}]?)[\"]?[,]?([NESWnesw]?),"
dd_regex_test <- "[\\{,][-]?[1]?[0-9]{1,2}\\.[0-9]{1,}[,]?[NESWnesw],"
#dd_regex <- "[\\{,][-]?[1]?[0-9]{1,2}\\.[0-9]{1,}[,]?[NESWnesw],"
#extract dates below
is_date_test <- str_detect(nlp_group_test$word, "BP")
#full_nlp_test$word[is_date_test&ird_word][4]
# use this for xx--xx BP picker identifier
date_range_test <- str_detect(nlp_group_test$word,
"(\\d+(?:[.]\\d+)*),((?:-{1,2})|(?:to)),(\\d+(?:[.]\\d+)*),([a-zA-Z]+,BP),")
date_match_test <- str_match(nlp_group_test$word,
"(\\d+(?:[.]\\d+)*),((?:-{1,2})|(?:to)),(\\d+(?:[.]\\d+)*),([a-zA-Z]+,BP),") %>% na.omit()
date_match_test2 <- str_extract(nlp_group_test$word,
"(\\d+(?:[.]\\d+)*),((?:-{1,2})|(?:to)),(\\d+(?:[.]\\d+)*),([a-zA-Z]+,BP),") %>% na.omit()
number_test <- str_detect(nlp_group_test$word,
",(\\d+(?:[\\.\\s]\\d+){0,1}),.*?,yr,")
number_match_test <- str_match(nlp_group_test$word, ",(\\d+(?:[\\.\\s]\\d+){0,1}),.*?,yr,")%>% na.omit()
number_match_test2 <- str_match(nlp_group_test$word, ",(\\d+(?:[\\.\\s]\\d+){0,1}),yr,")%>% na.omit()
# use this for yr picker identifier
number_test2 <- str_detect(nlp_group_test$word,
",(\\d+(?:[\\.\\s]\\d+){0,1}),yr,")
# use this for ka picker identifier
number_test3 <- str_detect(nlp_group_test$word,
",(\\d+(?:[\\.\\s]\\d+){0,1}),ka,")
# use this for BP picker identifier
number_test4 <- str_detect(nlp_group_test$word,
",(\\d+(?:[\\.\\s]\\d+){0,1}),BP,")
date_range_regex_test <- "(\\d+(?:[.]\\d+)*),((?:-{1,2})|(?:to)),(\\d+(?:[.]\\d+)*),([a-zA-Z]+,BP),"
date_regex_test <- ",(\\d+(?:[\\.\\s]\\d+){0,1}),.*?,yr,"
number <- str_detect(full_nlp_test$word,
",(\\d+(?:[\\.\\s]\\d+){0,1}),.*?,yr,")
degmin_test <- str_match_all(nlp_group_test$word, dms_regex_test)
decdeg_test <- str_match_all(nlp_group_test$word, dd_regex_test)
date_test <- str_match_all(nlp_group_test$word, date_regex_test)
date_range_test <- str_match_all(nlp_group_test$word, date_range_regex_test)
coord_pairs_test <- sapply(degmin_test, function(x)length(x) %% 2 == 0 & length(x) > 0) |
sapply(decdeg_test, function(x)length(x) %% 2 == 0 & length(x) > 0)
things <- nlp_group_test %>%
filter(coord_pairs_test) %>%
inner_join(publications_test, by = "_gddid") %>%
select(`_gddid`, word, year, title) %>%
mutate(word = gsub(',', ' ', word)) %>%
mutate(word = str_replace_all(word, '-LRB-', '(')) %>%
mutate(word = str_replace_all(word, '-RRB-', ')')) %>%
mutate(word = str_replace_all(word, '" "', ','))
things %>% select(-`_gddid`) %>% datatable
convert_dec <- function(x, i) {
drop_comma <- gsub(',', '', x) %>%
substr(., c(1,1), nchar(.) - 1) %>%
as.numeric %>%
unlist
domain <- (str_detect(x, 'N') * 1 +
str_detect(x, 'E') * 1 +
str_detect(x, 'W') * -1 +
str_detect(x, 'S') * -1) *
drop_comma
publ <- match(nlp_group_test$`_gddid`[i], publications_test$`_gddid`)
check_date <- function(input1, input2, input3, input4, index) {
if(isTRUE(input1[index])){
date_test_fun <- str_extract(nlp_group_test$word[index],
"(\\d+(?:[.]\\d+)*),((?:-{1,2})|(?:to)),(\\d+(?:[.]\\d+)*),([a-zA-Z]+,BP)," )
}
else if(isTRUE(input2[index])){
date_test_fun <- str_extract(nlp_group_test$word[index],
",(\\d+(?:[\\.\\s]\\d+){0,1}),yr,")
}
else if(isTRUE(input3[index])){
date_test_fun <- str_extract(nlp_group_test$word[index],
",(\\d+(?:[\\.\\s]\\d+){0,1}),ka,")
}
else if(isTRUE(input4[index])){
date_test_fun <- str_extract(nlp_group_test$word[index],
",(\\d+(?:[\\.\\s]\\d+){0,1}),BP,")
}
else {date_test_fun <- "NA"}
return(date_test_fun)
}
point_pairs_test <- data.frame(sentence = nlp_group_test$word[i],
lat = domain[str_detect(x, 'N') | str_detect(x, 'S')],
lng = domain[str_detect(x, 'E') | str_detect(x, 'W')],
publications_test[publ,],
date = check_date(date_range_test, number_test2, number_test3, number_test4, i),
stringsAsFactors = FALSE)
return(point_pairs_test)
}
convert_dm <- function(x, i) {
# We use the `i` index so that we can keep the coordinate outputs from the
#  regex in a smaller list.
dms_test <- data.frame(deg = as.numeric(x[,2]),
min = as.numeric(x[,3]) / 60,
sec = as.numeric(x[,4]) / 60 ^ 2,
stringsAsFactors = FALSE)
dms_test <- rowSums(dms_test, na.rm = TRUE)
domain <- (str_detect(x[,5], 'N') * 1 +
str_detect(x[,5], 'E') * 1 +
str_detect(x[,5], 'W') * -1 +
str_detect(x[,5], 'S') * -1) *
dms_test
publ <- match(nlp_group_test$`_gddid`[i], publications_test$`_gddid`)
check_date <- function(input1, input2, input3, input4, index) {
if(isTRUE(input1[index])){
date_test_fun <- str_extract(nlp_group_test$word[index],
"(\\d+(?:[.]\\d+)*),((?:-{1,2})|(?:to)),(\\d+(?:[.]\\d+)*),([a-zA-Z]+,BP)," )
}
else if(isTRUE(input2[index])){
date_test_fun <- str_extract(nlp_group_test$word[index],
",(\\d+(?:[\\.\\s]\\d+){0,1}),yr,")
}
else if(isTRUE(input3[index])){
date_test_fun <- str_extract(nlp_group_test$word[index],
",(\\d+(?:[\\.\\s]\\d+){0,1}),ka,")
}
else if(isTRUE(input4[index])){
date_test_fun <- str_extract(nlp_group_test$word[index],
",(\\d+(?:[\\.\\s]\\d+){0,1}),BP,")
}
else {date_test_fun <- "NA"}
return(date_test_fun)
}
point_pairs_test <- data.frame(sentence = nlp_group_test$word[i],
lat = domain[x[,5] %in% c('N', 'S')],
lng = domain[x[,5] %in% c('E', 'W')],
publications_test[publ,],
date = check_date(date_range_test, number_test2, number_test3, number_test4, i),
stringsAsFactors = FALSE)
return(point_pairs_test)
}
coordinates_test <- list()
coord_idx <- 1
for(i in 1:length(decdeg_test)) {
if((length(decdeg_test[[i]]) %% 2 == 0 |
length(degmin_test[[i]]) %% 2 == 0) & length(degmin_test[[i]]) > 0) {
if(any(str_detect(decdeg_test[[i]], '[NS]')) &
sum(str_detect(decdeg_test[[i]], '[EW]')) == sum(str_detect(decdeg_test[[i]], '[NS]'))) {
coordinates_test[[coord_idx]] <- convert_dec(decdeg_test[[i]], i)
coord_idx <- coord_idx + 1
}
if(any(str_detect(degmin_test[[i]], '[NS]')) &
sum(str_detect(degmin_test[[i]], '[EW]')) == sum(str_detect(degmin_test[[i]], '[NS]'))) {
coordinates_test[[coord_idx]] <- convert_dm(degmin_test[[i]], i)
coord_idx <- coord_idx + 1
}
}
}
coordinates_df_test_group_5s <- coordinates_test %>% bind_rows %>%
mutate(sentence = gsub(',', ' ', sentence)) %>%
mutate(sentence = str_replace_all(sentence, '-LRB-', '(')) %>%
mutate(sentence = str_replace_all(sentence, '-RRB-', ')')) %>%
mutate(sentence = str_replace_all(sentence, '" "', ','))
View(coordinates_df_test_group_5s)
coordinates_df_test_group_5s$doi <- coordinates_df_test_group_5s$identifier %>% map(function(x) x$id) %>% unlist
leaflet(coordinates_df_test_group_5s) %>%
addProviderTiles(providers$CartoDB.Positron) %>%
addCircleMarkers(popup = paste0('<b>', coordinates_df_test_group_5s$title, '</b><br>',
'<a href=https://doi.org/',
coordinates_df_test_group_5s$doi,'>Publication Link</a><br>',
'<b>Sentence:</b><br>',
'<small>',gsub(',', ' ', coordinates_df_test_group_5s$sentence),
'</small>'))
devtools::install_github('EarthCubeGeochron/geodiveR')
library(geodiveR)
library(jsonlite)
library(readr)
library(dplyr)
library(stringr)
library(leaflet)
library(purrr)
library(DT)
library(assertthat)
library(data.table)
sourcing <- list.files('R', full.names = TRUE) %>%
map(source, echo = FALSE, print = FALSE, verbose = FALSE)
publications_test <- fromJSON(txt = 'input/bibjson', flatten = TRUE)
full_nlp_test <- readr::read_tsv('input/sentences_nlp352',
trim_ws = TRUE,
col_names = c('_gddid', 'sentence', 'wordIndex',
'word', 'partofspeech', 'specialclass',
'wordsAgain', 'wordtype', 'wordmodified'))
nlp_clean_test <- clean_corpus(x = full_nlp_test, pubs = publications_test) #uses the clean_corpus.R function
nlp_test<-nlp_clean_test$nlp
#nlp_group_by_artical <- nlp_test
test_group <- NULL
test_group <- nlp_test
knitr::opts_chunk$set(echo = TRUE, message=FALSE, warning = FALSE)
library(geodiveR)
library(jsonlite)
library(readr)
library(dplyr)
library(stringr)
library(leaflet)
library(purrr)
library(DT)
library(assertthat)
sourcing <- list.files('R', full.names = TRUE) %>%
map(source, echo = FALSE, print = FALSE, verbose = FALSE)
publications <- fromJSON(txt = 'input/bibjson', flatten = TRUE)
full_nlp <- readr::read_tsv('input/sentences_nlp352',
trim_ws = TRUE,
col_names = c('_gddid', 'sentence', 'wordIndex',
'word', 'partofspeech', 'specialclass',
'wordsAgain', 'wordtype', 'wordmodified'))
nlp_clean <- clean_corpus(x = full_nlp, pubs = publications) #uses the clean_corpus.R function
nlp<-nlp_clean$nlp
regex.matching <- function(regEx, df, i){
returnLogic <- FALSE
detect <- str_detect(df$word[i], regEx)
if(detect){
returnLogic <- TRUE
}
cat("return logic looping", i)
return(returnLogic)
}
max.sentences(nlp,46051)
max.sentences(nlp,46050)
View(nlp)
max.sentences(nlp,42051)
max.sentences <- function(df, i){
returnValue <- df$sentence[i]
while((df$`_gddid`[i+1] ==df$`_gddid`[i]) && (i < nrow(df))){
returnValue <- returnValue + 1
i <- i+1
}
return(returnValue)
}
max.sentences(nlp,46050)
newFrame <- sentence.distribution(nlp, dms_regex, dd_regex, age_range_regex, age_yr_regex, age_yr2_regex, age_ka_regex, age_bp_regex)
